epoch: 1
100
139.46028470993042
200
297.4580156803131
300
450.0929584503174
400
618.8670465946198
500
791.2319855690002
600
943.3605642318726
Epoch: 1 | Training Loss: 0.560140 | Train ACC: 0.7532
epoch: 2
100
1212.996589422226
200
1379.4701933860779
300
1543.1412880420685
400
1695.3316595554352
500
1849.873130083084
600
2019.2764604091644
Epoch: 2 | Training Loss: 0.404479 | Train ACC: 0.8316
epoch: 3
100
2318.343479156494
200
2482.9587063789368
300
2617.634842634201
400
2738.3531908988953
500
2860.484798669815
600
2983.324536561966
Epoch: 3 | Training Loss: 0.332100 | Train ACC: 0.8605
epoch: 4
100
3210.4875638484955
200
3335.8160240650177
300
3460.2668073177338
400
3585.965688228607
500
3708.38191819191
600
3833.623639822006
Epoch: 4 | Training Loss: 0.282778 | Train ACC: 0.8833
epoch: 5
100
4099.672389507294
200
4270.689777612686
300
4431.767159461975
400
4599.062429904938
500
4759.845973491669
600
4917.366100072861
Epoch: 5 | Training Loss: 0.255164 | Train ACC: 0.9021
epoch: 6
100
5179.231414318085
200
5305.454967737198
300
5429.860132217407
400
5553.025611162186
500
5674.393443584442
600
5793.674186944962
Epoch: 6 | Training Loss: 0.230394 | Train ACC: 0.9076
epoch: 7
100
6011.561450481415
200
6150.585071563721
300
6311.984136104584
400
6476.063489437103
500
6636.243435382843
600
6784.36074090004
Epoch: 7 | Training Loss: 0.206696 | Train ACC: 0.9179
epoch: 8
100
7043.119401693344
200
7183.974866628647
300
7327.223178625107
400
7478.092716693878
500
7616.292736291885
600
7761.36602973938
Epoch: 8 | Training Loss: 0.176977 | Train ACC: 0.9312
epoch: 9
100
8027.8638916015625
200
8191.456020116806
300
8355.866864681244
400
8511.164580345154
500
8663.700690031052
600
8786.432389736176
Epoch: 9 | Training Loss: 0.171127 | Train ACC: 0.9330
epoch: 10
100
9008.212177753448
